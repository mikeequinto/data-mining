
##Michael Quinto et Jeremy Malera

### TP 2 ###

## setwd("C:/HEG/Data-mining/TPs/Project 2020-2021 Predict which investment product a client will buy-20201004")
## myData <- read.table("investing_program_prediction_data.csv", header=TRUE, sep=",")

##Exact call
##runAnalysis("investing_program_prediction_data.csv", c(2,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27),c(1,3,4,5,6,7,8,9),,28)


##Autres commandes utilisées

##Distribution de la probabilité de la variable cible InvType
## barplot(prop.table(table(myData[,28])))

## Exemple de distribution de probabilité de invType avec SE2 = G42
##  barplot(prop.table(table(myData[myData[,2]=="G42",28])))

## Exemple de distribution de probabilité de invType avec PE3 = I0
##  barplot(prop.table(table(myData[myData[,12]=="I0",28])))


##Pour les 3 attributs les plus importants

## Récupération de l'entropie conditionnelle de invType sachant attribut < seuil
##  conditionalEntropy.attribut_inf_seuil <- entropy(myData[myData[,attribut]<seuil, 28])
## Distribution de la probabilité de invType sachant attribut < seuil
##  barplot(prop.table(table(myData[myData[,attribut]<seuil,28])), main=paste("Conditional Entropy Of InvType Given attribut < seuil : ", conditionalEntropy.attribut_inf_seuil))

## Récupération de l'entropie conditionnelle de invType sachant attribut >= seuil
##  conditionalEntropy.attribute_sup_seuil <- entropy(myData[myData[,attribute]>=seuil, 28])
## Distribution de la probabilité de invType sachant attribute < seuil
##  barplot(prop.table(table(myData[myData[,attribute]>=seuil,28])), main=paste("Conditional Entropy Of InvType Given attribute >= seuil : ", conditionalEntropy.attribute_sup_seuil))

## Distribution de la probabilité de attribute selon le seuil
##  barplot(prop.table(table(as.factor(myData[,attribute] < seuil))), main="P(attribute binary)")


runAnalysis <- function(datasetNAme,
                        DiscreteAttributesIndeces,
                        ContinuousAttributesIndeces,
                        IndecesAttrsToRemoveIndeces,
                        ClassAttributeIndex) {
  
  
  ##Cette option permet d'afficher les valeurs réelles
  ##au lieu des notations scientifiques
  options(scipen=10000)
  
  ##Préparation du dataset
  dataset <- read.table(file=datasetNAme, header=TRUE, sep=",")
  
  ##highestInfoGainVariable <- ""
  ##highestInfoGainValue <- 0
  
  ##Pour chaque attribut discret
  for(i in DiscreteAttributesIndeces) {
    
    ##Nom de la variable
    varName <- names(dataset)[i]
    print(paste("Nom de la variable : ",varName))
    
    ##Récupération du nombre de valeurs unique
    nbValeurs <- length(unique(dataset[,i]))
    
    ##Entropie
    entropie <- entropy(dataset[,i])
    print(paste("Entropie : ",entropie))
    
    ##Entropie normalisée
    entropieNormalisee <- entropie / log2(nbValeurs)
    print(paste("Entropie normalisée : ", entropieNormalisee))
    
    ##Entropie conditionnelle
    entropieConditionnelle <- conditionalEntropy(dataset[,i],dataset[,ClassAttributeIndex])
    print(paste("Entropie conditionnelle : ",entropieConditionnelle))
    
    ##Information gain
    infoGain <- informationGain(dataset[,i],dataset[,ClassAttributeIndex])
    print(paste("Information gain : ",infoGain))
    
    ##Information gain ratio
    infoGainRatio <- infoGain / entropieNormalisee
    print(paste("Information gain ratio : ",infoGainRatio))
    
    ##Affichage des probabilités conditionnelles avec leur entropies conditionnelles
    ##conditionalEntropyLoop(dataset,i,ClassAttributeIndex)
    
    ##Affichage de la distribution de la probabilité de la variable cible selon la variable prédictive
    condProbDist <- prop.table(table(dataset[,c(ClassAttributeIndex,i)]))
    barplot(condProbDist, main=paste("Entropie de ",varName," : ",entropieNormalisee))
    
    print("")
    print("--------------------------------------------")
  }
  
  
  ##Pour chaque attribut continu
  for(j in ContinuousAttributesIndeces) {
    
    ##Nom de la variable
    varName <- names(dataset)[j]
    print(paste("Nom de la variable : ",varName))
    
    ##On récupère le seuil pour lequel l'information gain est maximale
    bestThreshold <- getBestThreshold(dataset[,j],dataset[,ClassAttributeIndex])
    
    ##On calcule l'information gain pour le seuil qu'on a récupéré
    infoGain <- informationGain(as.factor(dataset[,j] < bestThreshold), dataset[,ClassAttributeIndex])
    
    ##Calcul de l'information gain ratio
    infoGainRatio <- infoGain / entropy(as.factor(dataset[,j] < bestThreshold))
    
    print(paste("Le meilleur seuil a été trouvé pour t = ", bestThreshold))
    print(paste("Information gain = ",infoGain))
    print(paste("Information gain ratio = ",infoGainRatio))
    
    print("")
    print("--------------------------------------------")
  }
  
}

##Fonctions utiles

#x : variable prédictive
#y : variable cible

entropy <- function(x) {
  
  p <- prop.table(table(x))
  -sum(p*log2(p))
  
}

entropyNormalized <- function(x) {
  
  ##Récupération du nombre de valeurs unique
  nbValeurs <- length(unique(x))
  
  entropy(x) / log2(nbValeurs)
  
}

##La fonction ci-dessous peut être utilisé si les variables les plus importantes sont discrètes
#Conditional entropy du résultat sachant la variable H(resultat | sex=m) par exemple
conditionalEntropyLoop <- function(dataset, x, y) {
  
  ## x = indice de la variable prédictive
  ## y = indice de la variable cible
  
  ##Récupération des valeurs de la variable
  ##On met dans l'ordre croissant les valeurs uniques
  orderedList <- sort(unique(dataset[,x]))
  
  ##Pour chaque valeur
  for(i in orderedList){
    
    ##Récupération de la probabilité conditionnelle
    probDistCond <- prop.table(table(dataset[dataset[,x]==i,y]))
    
    ##Calcule de l'entropie conditionnelle selon la valeur
    condEntrResGivenValue <- entropy(dataset[dataset[,x]==i,y])
    
    ##Visualisation de la probabilité conditionnelle avec un barplot
    barplot(probDistCond,main=paste("Conditional Entropy of ",
                                     names(dataset)[y]," Given ",
                                     names(dataset)[x],"==",i," : ", 
                                     condEntrResGivenValue))
  }
  
}

conditionalEntropy <- function(x,y){
  condProp <- prop.table(table(x,y),1)
  -sum(rowSums(condProp * log2(condProp)) * prop.table(table(x)), na.rm=TRUE)
}


informationGain <- function(x,y){
  
  entropy(y) - conditionalEntropy(x,y)
  
}

## Fonction permettant de trouver le seuil pour lequel l'information gain pour la variable est maximale
getBestThreshold <- function(x,y) {
  
  ##On récupère les valeurs uniques de la variable
  ##On les met dans l'ordre croissant
  sortedUnique <- sort(unique(x))
  
  ##Récupération du nombre de valeurs unique
  nbValeurs <- length(sortedUnique)
  
  bestThreshold <- 1
  bestInfoGain <- 0
  
  ##Pour chaque valeur
  for(i in sortedUnique){
    
    ##On calcule l'information gain pour le seuil t = i
    infoGain <- informationGain(as.factor(x < i), y)
    
    ##On garde l'information gain pour le seuil t si elle est supérieure
    if(infoGain > bestInfoGain){
      bestThreshold = i
      bestInfoGain = infoGain
    }

  }
  
  ##On retourne le meilleur seuil
  bestThreshold
  
}

