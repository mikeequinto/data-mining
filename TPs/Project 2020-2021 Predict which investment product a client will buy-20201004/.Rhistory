##entropy(myData[,1]) par exemple
p <- prop.table(table(x))
-sum(p*log2(p))
}
entropyNormalized <- function(x) {
##Récupération du nombre de valeurs unique
nbValeurs <- length(unique(x))
entropy(x) / log2(nbValeurs)
}
##La fonction ci-dessous peut être utilisé si les variables les plus importantes sont discrètes
#Conditional entropy du résultat sachant la variable H(resultat | sex=m) par exemple
conditionalEntropyLoop <- function(dataset, x, y) {
## x = indice de la variable prédictive
## y = indice de la variable cible
##Récupération des valeurs de la variable
##On met dans l'ordre croissant les valeurs uniques
orderedList <- sort(unique(dataset[,x]))
##Pour chaque valeur
for(i in orderedList){
##Récupération de la probabilité conditionnelle
probDistCond <- prop.table(table(dataset[dataset[,x]==i,y]))
##Calcule de l'entropie conditionnelle selon la valeur
condEntrResGivenValue <- entropy(dataset[dataset[,x]==i,y])
##Visualisation de la probabilité conditionnelle avec un barplot
barplot(probDistCond,main=paste("Conditional Entropy of ",
names(dataset)[y]," Given ",
names(dataset)[x],"==",i," : ",
condEntrResGivenValue))
}
}
conditionalEntropy <- function(x,y){
condProp <- prop.table(table(x,y),1)
-sum(rowSums(condProp * log2(condProp)) * prop.table(table(x)), na.rm=TRUE)
}
#y : variable cible
#x : variable prédictive
informationGain <- function(x,y){
entropy(y) - conditionalEntropy(x,y)
}
## Fonction permettant de trouver le seuil pour lequel l'information gain pour la variable est maximale
getBestThreshold <- function(x,y) {
##On récupère les valeurs uniques de la variable
##On les met dans l'ordre croissant
sortedUnique <- sort(unique(x))
##Récupération du nombre de valeurs unique
nbValeurs <- length(sortedUnique)
bestThreshold <- 1
bestInfoGain <- 0
##Pour chaque valeur
for(i in sortedUnique){
##On calcule l'information gain pour le seuil t = i
infoGain <- informationGain(as.factor(x < i), y)
##On garde l'information gain pour le seuil t si elle est supérieure
if(infoGain > bestInfoGain){
bestThreshold = i
bestInfoGain = infoGain
}
}
##On retourne le meilleur seuil
bestThreshold
}
runAnalysis("investing_program_prediction_data.csv", c(2,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27),c(1,3,4,5,6,7,8,9),,28)
runAnalysis("investing_program_prediction_data.csv", c(2,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27),c(1,3,4,5,6,7,8,9),,28)
### TP 2 ###
## setwd("C:/HEG/Data-mining/TPs/Project 2020-2021 Predict which investment product a client will buy-20201004")
## myData <- read.table("investing_program_prediction_data.csv", header=TRUE, sep=",")
##Exact call
##runAnalysis("investing_program_prediction_data.csv", c(2,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27),c(1,3,4,5,6,7,8,9),,28)
##Autres commandes utilisées
##Attribut ayant l'information gain le plus élevé parmis les attributs discrets
## Distribution de la probabilité de invType sachant la valeur de SE2
##  barplot(prop.table(table(myData[,c(28,2)])), legend.text=T, main=paste("Entropie normalisée de SE2 : ",entropyNormalized(myData[,2])))
## Exemple de distribution de probabilité de invType avec SE2 = G42
##  barplot(prop.table(table(myData[myData[,2]=="G42",28])))
##Attribut ayant l'information gain le plus bas parmis les attributs discrets
## Distribution de la probabilité de invType sachant la valeur de PE3
##  barplot(prop.table(table(myData[,c(28,12)])), legend.text=T, main=paste("Entropie de PE3 : ",entropy(myData[,12])))
## Exemple de distribution de probabilité de invType avec PE3 = I0
##  barplot(prop.table(table(myData[myData[,12]=="I0",28])))
##Pour les 3 attributs les plus importants
## Récupération de l'entropie conditionnelle de invType sachant attribut < seuil
##  conditionalEntropy.attribut_inf_seuil <- entropy(myData[myData[,attribut]<seuil, 28])
## Distribution de la probabilité de invType sachant attribut < seuil
##  barplot(prop.table(table(myData[myData[,attribut]<seuil,28])), main=paste("Conditional Entropy Of InvType Given attribut < seuil : ", conditionalEntropy.attribut_inf_seuil))
## Récupération de l'entropie conditionnelle de invType sachant attribut >= seuil
##  conditionalEntropy.attribute_sup_seuil <- entropy(myData[myData[,attribute]>=seuil, 28])
## Distribution de la probabilité de invType sachant attribute < seuil
##  barplot(prop.table(table(myData[myData[,attribute]>=seuil,28])), main=paste("Conditional Entropy Of InvType Given attribute >= seuil : ", conditionalEntropy.attribute_sup_seuil))
## Distribution de la probabilité de attribute selon le seuil
##  barplot(prop.table(table(as.factor(myData[,attribute] < seuil))), main="P(attribute binary)")
runAnalysis <- function(datasetNAme,
DiscreteAttributesIndeces,
ContinuousAttributesIndeces,
IndecesAttrsToRemoveIndeces,
ClassAttributeIndex) {
##Cette option permet d'afficher les valeurs réelles
##au lieu des notations scientifiques
options(scipen=10000)
##Préparation du dataset
dataset <- read.table(file=datasetNAme, header=TRUE, sep=",")
##highestInfoGainVariable <- ""
##highestInfoGainValue <- 0
##Pour chaque attribut discret
for(i in DiscreteAttributesIndeces) {
##Nom de la variable
varName <- names(dataset)[i]
print(paste("Nom de la variable : ",varName))
##Récupération du nombre de valeurs unique
nbValeurs <- length(unique(dataset[,i]))
##Entropie
entropie <- entropy(dataset[,i])
print(paste("Entropie : ",entropie))
##Entropie normalisée
entropieNormalisee <- entropie / log2(nbValeurs)
print(paste("Entropie normalisée : ", entropieNormalisee))
##Entropie conditionnelle
entropieConditionnelle <- conditionalEntropy(dataset[,i],dataset[,ClassAttributeIndex])
print(paste("Entropie conditionnelle : ",entropieConditionnelle))
##Information gain
infoGain <- informationGain(dataset[,i],dataset[,ClassAttributeIndex])
print(paste("Information gain : ",infoGain))
##Information gain ratio
infoGainRatio <- infoGain / entropieNormalisee
print(paste("Information gain ratio : ",infoGainRatio))
##Affichage des probabilités conditionnelles avec leur entropies conditionnelles
##conditionalEntropyLoop(dataset,i,ClassAttributeIndex)
##Affichage de la distribution de la probabilité de la variable cible selon la variable prédictive
condProbDist <- prop.table(table(dataset[,c(ClassAttributeIndex,i)]))
barplot(condProbDist, main=paste("Entropie de ",varName," : ",entropieNormalisee))
print("")
print("--------------------------------------------")
}
##Pour chaque attribut continu
for(j in ContinuousAttributesIndeces) {
##Nom de la variable
varName <- names(dataset)[j]
print(paste("Nom de la variable : ",varName))
##On récupère le seuil pour lequel l'information gain est maximale
bestThreshold <- getBestThreshold(dataset[,j],dataset[,ClassAttributeIndex])
##On calcule l'information gain pour le seuil qu'on a récupéré
infoGain <- informationGain(as.factor(dataset[,j] < bestThreshold), dataset[,ClassAttributeIndex])
##Calcul de l'information gain ratio
infoGainRatio <- infoGain / entropy(as.factor(dataset[,j] < bestThreshold))
print(paste("Le meilleur seuil a été trouvé pour t = ", bestThreshold))
print(paste("Information gain = ",infoGain))
print(paste("Information gain ratio = ",infoGainRatio))
print("")
print("--------------------------------------------")
}
}
entropy <- function(x) {
##entropy(myData[,1]) par exemple
p <- prop.table(table(x))
-sum(p*log2(p))
}
entropyNormalized <- function(x) {
##Récupération du nombre de valeurs unique
nbValeurs <- length(unique(x))
entropy(x) / log2(nbValeurs)
}
##La fonction ci-dessous peut être utilisé si les variables les plus importantes sont discrètes
#Conditional entropy du résultat sachant la variable H(resultat | sex=m) par exemple
conditionalEntropyLoop <- function(dataset, x, y) {
## x = indice de la variable prédictive
## y = indice de la variable cible
##Récupération des valeurs de la variable
##On met dans l'ordre croissant les valeurs uniques
orderedList <- sort(unique(dataset[,x]))
##Pour chaque valeur
for(i in orderedList){
##Récupération de la probabilité conditionnelle
probDistCond <- prop.table(table(dataset[dataset[,x]==i,y]))
##Calcule de l'entropie conditionnelle selon la valeur
condEntrResGivenValue <- entropy(dataset[dataset[,x]==i,y])
##Visualisation de la probabilité conditionnelle avec un barplot
barplot(probDistCond,main=paste("Conditional Entropy of ",
names(dataset)[y]," Given ",
names(dataset)[x],"==",i," : ",
condEntrResGivenValue))
}
}
conditionalEntropy <- function(x,y){
condProp <- prop.table(table(x,y),1)
-sum(rowSums(condProp * log2(condProp)) * prop.table(table(x)), na.rm=TRUE)
}
#y : variable cible
#x : variable prédictive
informationGain <- function(x,y){
entropy(y) - conditionalEntropy(x,y)
}
## Fonction permettant de trouver le seuil pour lequel l'information gain pour la variable est maximale
getBestThreshold <- function(x,y) {
##On récupère les valeurs uniques de la variable
##On les met dans l'ordre croissant
sortedUnique <- sort(unique(x))
##Récupération du nombre de valeurs unique
nbValeurs <- length(sortedUnique)
bestThreshold <- 1
bestInfoGain <- 0
##Pour chaque valeur
for(i in sortedUnique){
##On calcule l'information gain pour le seuil t = i
infoGain <- informationGain(as.factor(x < i), y)
##On garde l'information gain pour le seuil t si elle est supérieure
if(infoGain > bestInfoGain){
bestThreshold = i
bestInfoGain = infoGain
}
}
##On retourne le meilleur seuil
bestThreshold
}
runAnalysis("investing_program_prediction_data.csv", c(2,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27),c(1,3,4,5,6,7,8,9),,28)
barplot(prop.table(table(myData[,28])))
informationGain(myData[,2],myData[,28])
informationGain(myData[,2],myData[,28]) / entropy(myData[,2])
##Michael Quinto et Jeremy Malera
### TP 2 ###
## setwd("C:/HEG/Data-mining/TPs/Project 2020-2021 Predict which investment product a client will buy-20201004")
## myData <- read.table("investing_program_prediction_data.csv", header=TRUE, sep=",")
##Exact call
##runAnalysis("investing_program_prediction_data.csv", c(2,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27),c(1,3,4,5,6,7,8,9),,28)
##Autres commandes utilisées
##Distribution de la probabilité de la variable cible InvType
## barplot(prop.table(table(myData[,28])))
## Exemple de distribution de probabilité de invType avec SE2 = G42
##  barplot(prop.table(table(myData[myData[,2]=="G42",28])))
## Exemple de distribution de probabilité de invType avec PE3 = I0
##  barplot(prop.table(table(myData[myData[,12]=="I0",28])))
##Pour les 3 attributs les plus importants
## Récupération de l'entropie conditionnelle de invType sachant attribut < seuil
##  conditionalEntropy.attribut_inf_seuil <- entropy(myData[myData[,attribut]<seuil, 28])
## Distribution de la probabilité de invType sachant attribut < seuil
##  barplot(prop.table(table(myData[myData[,attribut]<seuil,28])), main=paste("Conditional Entropy Of InvType Given attribut < seuil : ", conditionalEntropy.attribut_inf_seuil))
## Récupération de l'entropie conditionnelle de invType sachant attribut >= seuil
##  conditionalEntropy.attribute_sup_seuil <- entropy(myData[myData[,attribute]>=seuil, 28])
## Distribution de la probabilité de invType sachant attribute < seuil
##  barplot(prop.table(table(myData[myData[,attribute]>=seuil,28])), main=paste("Conditional Entropy Of InvType Given attribute >= seuil : ", conditionalEntropy.attribute_sup_seuil))
## Distribution de la probabilité de attribute selon le seuil
##  barplot(prop.table(table(as.factor(myData[,attribute] < seuil))), main="P(attribute binary)")
runAnalysis <- function(datasetNAme,
DiscreteAttributesIndeces,
ContinuousAttributesIndeces,
IndecesAttrsToRemoveIndeces,
ClassAttributeIndex) {
##Cette option permet d'afficher les valeurs réelles
##au lieu des notations scientifiques
options(scipen=10000)
##Préparation du dataset
dataset <- read.table(file=datasetNAme, header=TRUE, sep=",")
##highestInfoGainVariable <- ""
##highestInfoGainValue <- 0
##Pour chaque attribut discret
for(i in DiscreteAttributesIndeces) {
##Nom de la variable
varName <- names(dataset)[i]
print(paste("Nom de la variable : ",varName))
##Récupération du nombre de valeurs unique
nbValeurs <- length(unique(dataset[,i]))
##Entropie
entropie <- entropy(dataset[,i])
print(paste("Entropie : ",entropie))
##Entropie normalisée
entropieNormalisee <- entropie / log2(nbValeurs)
print(paste("Entropie normalisée : ", entropieNormalisee))
##Entropie conditionnelle
entropieConditionnelle <- conditionalEntropy(dataset[,i],dataset[,ClassAttributeIndex])
print(paste("Entropie conditionnelle : ",entropieConditionnelle))
##Information gain
infoGain <- informationGain(dataset[,i],dataset[,ClassAttributeIndex])
print(paste("Information gain : ",infoGain))
##Information gain ratio
infoGainRatio <- infoGain / entropie
print(paste("Information gain ratio : ",infoGainRatio))
##Affichage des probabilités conditionnelles avec leur entropies conditionnelles
##conditionalEntropyLoop(dataset,i,ClassAttributeIndex)
##Affichage de la distribution de la probabilité de la variable cible selon la variable prédictive
condProbDist <- prop.table(table(dataset[,c(ClassAttributeIndex,i)]))
barplot(condProbDist, main=paste("Entropie de ",varName," : ",entropieNormalisee))
print("")
print("--------------------------------------------")
}
##Pour chaque attribut continu
for(j in ContinuousAttributesIndeces) {
##Nom de la variable
varName <- names(dataset)[j]
print(paste("Nom de la variable : ",varName))
##On récupère le seuil pour lequel l'information gain est maximale
bestThreshold <- getBestThreshold(dataset[,j],dataset[,ClassAttributeIndex])
##On calcule l'information gain pour le seuil qu'on a récupéré
infoGain <- informationGain(as.factor(dataset[,j] < bestThreshold), dataset[,ClassAttributeIndex])
##Calcul de l'information gain ratio
infoGainRatio <- infoGain / entropy(as.factor(dataset[,j] < bestThreshold))
print(paste("Le meilleur seuil a été trouvé pour t = ", bestThreshold))
print(paste("Information gain = ",infoGain))
print(paste("Information gain ratio = ",infoGainRatio))
print("")
print("--------------------------------------------")
}
}
##Fonctions utiles
#x : variable prédictive
#y : variable cible
entropy <- function(x) {
p <- prop.table(table(x))
-sum(p*log2(p))
}
entropyNormalized <- function(x) {
##Récupération du nombre de valeurs unique
nbValeurs <- length(unique(x))
entropy(x) / log2(nbValeurs)
}
##La fonction ci-dessous peut être utilisé si les variables les plus importantes sont discrètes
#Conditional entropy du résultat sachant la variable H(resultat | sex=m) par exemple
conditionalEntropyLoop <- function(dataset, x, y) {
## x = indice de la variable prédictive
## y = indice de la variable cible
##Récupération des valeurs de la variable
##On met dans l'ordre croissant les valeurs uniques
orderedList <- sort(unique(dataset[,x]))
##Pour chaque valeur
for(i in orderedList){
##Récupération de la probabilité conditionnelle
probDistCond <- prop.table(table(dataset[dataset[,x]==i,y]))
##Calcule de l'entropie conditionnelle selon la valeur
condEntrResGivenValue <- entropy(dataset[dataset[,x]==i,y])
##Visualisation de la probabilité conditionnelle avec un barplot
barplot(probDistCond,main=paste("Conditional Entropy of ",
names(dataset)[y]," Given ",
names(dataset)[x],"==",i," : ",
condEntrResGivenValue))
}
}
conditionalEntropy <- function(x,y){
condProp <- prop.table(table(x,y),1)
-sum(rowSums(condProp * log2(condProp)) * prop.table(table(x)), na.rm=TRUE)
}
informationGain <- function(x,y){
entropy(y) - conditionalEntropy(x,y)
}
## Fonction permettant de trouver le seuil pour lequel l'information gain pour la variable est maximale
getBestThreshold <- function(x,y) {
##On récupère les valeurs uniques de la variable
##On les met dans l'ordre croissant
sortedUnique <- sort(unique(x))
##Récupération du nombre de valeurs unique
nbValeurs <- length(sortedUnique)
bestThreshold <- 1
bestInfoGain <- 0
##Pour chaque valeur
for(i in sortedUnique){
##On calcule l'information gain pour le seuil t = i
infoGain <- informationGain(as.factor(x < i), y)
##On garde l'information gain pour le seuil t si elle est supérieure
if(infoGain > bestInfoGain){
bestThreshold = i
bestInfoGain = infoGain
}
}
##On retourne le meilleur seuil
bestThreshold
}
runAnalysis("investing_program_prediction_data.csv", c(2,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27),c(1,3,4,5,6,7,8,9),,28)
##Michael Quinto et Jeremy Malera
### TP 2 ###
## setwd("C:/HEG/Data-mining/TPs/Project 2020-2021 Predict which investment product a client will buy-20201004")
## myData <- read.table("investing_program_prediction_data.csv", header=TRUE, sep=",")
##Exact call
##runAnalysis("investing_program_prediction_data.csv", c(2,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27),c(1,3,4,5,6,7,8,9),,28)
##Autres commandes utilisées
##Distribution de la probabilité de la variable cible InvType
## barplot(prop.table(table(myData[,28])))
## Exemple de distribution de probabilité de invType avec SE2 = G42
##  barplot(prop.table(table(myData[myData[,2]=="G42",28])))
## Exemple de distribution de probabilité de invType avec PE3 = I0
##  barplot(prop.table(table(myData[myData[,12]=="I0",28])))
##Pour les 3 attributs les plus importants
## Récupération de l'entropie conditionnelle de invType sachant attribut < seuil
##  conditionalEntropy.attribut_inf_seuil <- entropy(myData[myData[,attribut]<seuil, 28])
## Distribution de la probabilité de invType sachant attribut < seuil
##  barplot(prop.table(table(myData[myData[,attribut]<seuil,28])), main=paste("Conditional Entropy Of InvType Given attribut < seuil : ", conditionalEntropy.attribut_inf_seuil))
## Récupération de l'entropie conditionnelle de invType sachant attribut >= seuil
##  conditionalEntropy.attribute_sup_seuil <- entropy(myData[myData[,attribute]>=seuil, 28])
## Distribution de la probabilité de invType sachant attribute < seuil
##  barplot(prop.table(table(myData[myData[,attribute]>=seuil,28])), main=paste("Conditional Entropy Of InvType Given attribute >= seuil : ", conditionalEntropy.attribute_sup_seuil))
## Distribution de la probabilité de attribute selon le seuil
##  barplot(prop.table(table(as.factor(myData[,attribute] < seuil))), main="P(attribute binary)")
runAnalysis <- function(datasetNAme,
DiscreteAttributesIndeces,
ContinuousAttributesIndeces,
IndecesAttrsToRemoveIndeces,
ClassAttributeIndex) {
##Cette option permet d'afficher les valeurs réelles
##au lieu des notations scientifiques
options(scipen=10000)
##Préparation du dataset
dataset <- read.table(file=datasetNAme, header=TRUE, sep=",")
##highestInfoGainVariable <- ""
##highestInfoGainValue <- 0
##Pour chaque attribut discret
for(i in DiscreteAttributesIndeces) {
##Nom de la variable
varName <- names(dataset)[i]
print(paste("Nom de la variable : ",varName))
##Récupération du nombre de valeurs unique
nbValeurs <- length(unique(dataset[,i]))
##Entropie
entropie <- entropy(dataset[,i])
print(paste("Entropie : ",entropie))
##Entropie normalisée
entropieNormalisee <- entropie / log2(nbValeurs)
print(paste("Entropie normalisée : ", entropieNormalisee))
##Entropie conditionnelle
entropieConditionnelle <- conditionalEntropy(dataset[,i],dataset[,ClassAttributeIndex])
print(paste("Entropie conditionnelle : ",entropieConditionnelle))
##Information gain
infoGain <- informationGain(dataset[,i],dataset[,ClassAttributeIndex])
print(paste("Information gain : ",infoGain))
##Information gain ratio
infoGainRatio <- infoGain / entropieNormalisee
print(paste("Information gain ratio : ",infoGainRatio))
##Affichage des probabilités conditionnelles avec leur entropies conditionnelles
##conditionalEntropyLoop(dataset,i,ClassAttributeIndex)
##Affichage de la distribution de la probabilité de la variable cible selon la variable prédictive
condProbDist <- prop.table(table(dataset[,c(ClassAttributeIndex,i)]))
barplot(condProbDist, main=paste("Entropie de ",varName," : ",entropieNormalisee))
print("")
print("--------------------------------------------")
}
##Pour chaque attribut continu
for(j in ContinuousAttributesIndeces) {
##Nom de la variable
varName <- names(dataset)[j]
print(paste("Nom de la variable : ",varName))
##On récupère le seuil pour lequel l'information gain est maximale
bestThreshold <- getBestThreshold(dataset[,j],dataset[,ClassAttributeIndex])
##On calcule l'information gain pour le seuil qu'on a récupéré
infoGain <- informationGain(as.factor(dataset[,j] < bestThreshold), dataset[,ClassAttributeIndex])
##Calcul de l'information gain ratio
infoGainRatio <- infoGain / entropy(as.factor(dataset[,j] < bestThreshold))
print(paste("Le meilleur seuil a été trouvé pour t = ", bestThreshold))
print(paste("Information gain = ",infoGain))
print(paste("Information gain ratio = ",infoGainRatio))
print("")
print("--------------------------------------------")
}
}
##Fonctions utiles
#x : variable prédictive
#y : variable cible
entropy <- function(x) {
p <- prop.table(table(x))
-sum(p*log2(p))
}
entropyNormalized <- function(x) {
##Récupération du nombre de valeurs unique
nbValeurs <- length(unique(x))
entropy(x) / log2(nbValeurs)
}
##La fonction ci-dessous peut être utilisé si les variables les plus importantes sont discrètes
#Conditional entropy du résultat sachant la variable H(resultat | sex=m) par exemple
conditionalEntropyLoop <- function(dataset, x, y) {
## x = indice de la variable prédictive
## y = indice de la variable cible
##Récupération des valeurs de la variable
##On met dans l'ordre croissant les valeurs uniques
orderedList <- sort(unique(dataset[,x]))
##Pour chaque valeur
for(i in orderedList){
##Récupération de la probabilité conditionnelle
probDistCond <- prop.table(table(dataset[dataset[,x]==i,y]))
##Calcule de l'entropie conditionnelle selon la valeur
condEntrResGivenValue <- entropy(dataset[dataset[,x]==i,y])
##Visualisation de la probabilité conditionnelle avec un barplot
barplot(probDistCond,main=paste("Conditional Entropy of ",
names(dataset)[y]," Given ",
names(dataset)[x],"==",i," : ",
condEntrResGivenValue))
}
}
conditionalEntropy <- function(x,y){
condProp <- prop.table(table(x,y),1)
-sum(rowSums(condProp * log2(condProp)) * prop.table(table(x)), na.rm=TRUE)
}
informationGain <- function(x,y){
entropy(y) - conditionalEntropy(x,y)
}
## Fonction permettant de trouver le seuil pour lequel l'information gain pour la variable est maximale
getBestThreshold <- function(x,y) {
##On récupère les valeurs uniques de la variable
##On les met dans l'ordre croissant
sortedUnique <- sort(unique(x))
##Récupération du nombre de valeurs unique
nbValeurs <- length(sortedUnique)
bestThreshold <- 1
bestInfoGain <- 0
##Pour chaque valeur
for(i in sortedUnique){
##On calcule l'information gain pour le seuil t = i
infoGain <- informationGain(as.factor(x < i), y)
##On garde l'information gain pour le seuil t si elle est supérieure
if(infoGain > bestInfoGain){
bestThreshold = i
bestInfoGain = infoGain
}
}
##On retourne le meilleur seuil
bestThreshold
}
runAnalysis("investing_program_prediction_data.csv", c(2,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27),c(1,3,4,5,6,7,8,9),,28)
save.image("C:/HEG/Data-mining/TPs/Project 2020-2021 Predict which investment product a client will buy-20201004/.RData")
informationGain(myData[,2],myData[,28]) / entropy(myData[,2])
